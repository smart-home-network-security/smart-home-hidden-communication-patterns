{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f7b5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Initialization.\n",
    "Import libraries, and define global variables.\n",
    "\"\"\"\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Imports\n",
    "import os\n",
    "import glob\n",
    "from natsort import natsorted\n",
    "\n",
    "\n",
    "def must_process_dir(dir: str) -> bool:\n",
    "    \"\"\"\n",
    "    Check if given directory must be processed,\n",
    "    i.e. if it contains traces to analyze.\n",
    "\n",
    "    Args:\n",
    "        dir (str): path to the directory to check.\n",
    "    Returns:\n",
    "        bool: True if the directory must be processed, False otherwise.\n",
    "    \"\"\"\n",
    "    if dir.is_dir():\n",
    "        files = glob.glob(os.path.join(dir.path, \"traces\", \"*.pcap\"))\n",
    "        return len(files) > 0\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "##### CONFIG #####\n",
    "\n",
    "## Paths\n",
    "BASE_DIR = os.getcwd()\n",
    "## Device under test\n",
    "# Update with your device's details\n",
    "DEVICE_CATEGORY = \"plugs\"\n",
    "DEVICE_NAME = \"TpLinkPlugTapo\"\n",
    "DEVICE_MAC = \"50:c7:bf:ed:0a:54\"\n",
    "DEVICE_IP = \"192.168.1.135\"\n",
    "DEVICE_INTERFACE = \"wlan1\"\n",
    "DEVICE_DIR = os.path.join(BASE_DIR, \"data\", \"pcap\", DEVICE_CATEGORY, DEVICE_NAME)\n",
    "## Event under test\n",
    "HEURISTIC = \"node_pruning\"\n",
    "EVENT = \"toggle\"  # Update with the event under test\n",
    "EVENT_DIR = os.path.join(DEVICE_DIR, HEURISTIC, EVENT)\n",
    "# Policies\n",
    "policy_dirs = natsorted([f.path for f in os.scandir(EVENT_DIR) if must_process_dir(f)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2e25a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "For each PCAP file, extract packets.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "\n",
    "from signature_extraction.pkt_extraction import pcap_to_pkts, pkts_to_csv\n",
    "\n",
    "\n",
    "def get_policy_name_from_path(policy_dir):\n",
    "    basename = os.path.basename(policy_dir)\n",
    "    return basename.partition(\"_\")[2]\n",
    "\n",
    "\n",
    "# Read initial DNS table\n",
    "path_dns_table = os.path.join(EVENT_DIR, \"0_root\", \"dns_table.json\")\n",
    "try:\n",
    "    with open(path_dns_table, \"r\") as f:\n",
    "        dns_table = json.load(f)\n",
    "except FileNotFoundError:\n",
    "    print(f\"DNS table not found at {path_dns_table}. Using empty DNS table.\")\n",
    "    dns_table = {}\n",
    "\n",
    "pcaps_per_policy = {}\n",
    "pkts_per_policy = {}\n",
    "for policy_dir in policy_dirs:\n",
    "    policy_name = os.path.basename(policy_dir)\n",
    "    print(f\"Policy: {policy_name}\")\n",
    "    print()\n",
    "    policy_pkts = []\n",
    "    pkts_per_policy[policy_name] = policy_pkts\n",
    "\n",
    "    # Path(s) to PCAP file(s)\n",
    "    traces_dir = os.path.join(policy_dir, \"traces\")\n",
    "    pcaps = glob.glob(f\"{traces_dir}/*.pcap\")\n",
    "    pcaps_per_policy[policy_name] = pcaps\n",
    "\n",
    "    # Extract packets\n",
    "    pkts_matrix = []\n",
    "    for i, pcap in enumerate(pcaps):\n",
    "        # Read packets\n",
    "        pkts = pcap_to_pkts(pcap, dns_table)\n",
    "        policy_pkts.append(pkts)\n",
    "        # Save packets to CSV\n",
    "        csv_file_path = pcap.replace(\".pcap\", \".csv\")\n",
    "        pkts_to_csv(pkts, csv_file_path)\n",
    "        # Print packets\n",
    "        print(f\"PCAP #{i+1}: {len(pkts)} packets ({pcap})\")\n",
    "        # print(\"\\n\".join([str(pkt) for pkt in pkts]))\n",
    "        print()\n",
    "    \n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1611eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "For each PCAP, group packets per bidirectional flow,\n",
    "i.e. packets having the same:\n",
    "    - IP addresses\n",
    "    - Ports\n",
    "    - Transport protocol\n",
    "\n",
    "As flows are *bidirectional*, packets in both directions corresponding to the same data exchange are grouped. \n",
    "\"\"\"\n",
    "\n",
    "from signature_extraction.flow_grouping import group_pkts_per_flow\n",
    "\n",
    "\n",
    "# Group packets per flow\n",
    "patterns_per_policy = {}\n",
    "for policy_name, pkts_matrix in pkts_per_policy.items():\n",
    "    print(f\"Policy: {policy_name}\")\n",
    "    print()\n",
    "    patterns = []\n",
    "    patterns_per_policy[policy_name] = patterns\n",
    "    for i, pkts in enumerate(pkts_matrix):\n",
    "        # Group packets per flow\n",
    "        pattern = group_pkts_per_flow(pkts)\n",
    "        patterns.append(pattern)\n",
    "        # Save flows as CSV\n",
    "        policy_pcap = pcaps_per_policy[policy_name][i]\n",
    "        pattern_csv_path = policy_pcap.replace(\".pcap\", \"_flows.csv\")\n",
    "        pattern.to_csv(pattern_csv_path)\n",
    "        # Display pattern\n",
    "        print(f\"Pattern {i+1}: ({policy_pcap})\")\n",
    "        print(pattern)\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917f95ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Sort network patterns per length.\n",
    "\"\"\"\n",
    "\n",
    "for policy_name, patterns in patterns_per_policy.items():\n",
    "    print(f\"Policy: {policy_name}\")\n",
    "    print()\n",
    "    sorted_patterns = sorted(patterns, key=len)\n",
    "    for i, pattern in enumerate(sorted_patterns):\n",
    "        policy_pcap = pcaps_per_policy[policy_name][i]\n",
    "        print(f\"Pattern {i+1}: ({policy_pcap})\")\n",
    "        print(pattern)\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3795fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Extract event signature from the list of patterns.\n",
    "\"\"\"\n",
    "\n",
    "from signature_extraction.event_signature_extraction import patterns_to_signature\n",
    "\n",
    "\n",
    "signature_per_policy = {}\n",
    "for policy_name, patterns in patterns_per_policy.items():\n",
    "    print(f\"Policy: {policy_name}\")\n",
    "\n",
    "    # Skip policy if no pattern\n",
    "    if len(patterns) == 0:\n",
    "        continue\n",
    "\n",
    "    # Extract event signature\n",
    "    signature = patterns_to_signature(patterns)\n",
    "    signature_per_policy[policy_name] = signature\n",
    "\n",
    "    # Save signature as CSV file\n",
    "    policy_dir = os.path.join(EVENT_DIR, policy_name)\n",
    "    output_csv_path = os.path.join(policy_dir, \"signature.csv\")\n",
    "    signature.to_csv(output_csv_path)\n",
    "\n",
    "    # Display signature\n",
    "    print(signature)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28897e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Extract a profile-compliant policy from the event signature.\n",
    "\"\"\"\n",
    "\n",
    "from pprint import pprint\n",
    "import json\n",
    "from utils.heuristic import contains_policy\n",
    "\n",
    "\n",
    "def policy_exists(policy: dict, all_policies: dict) -> bool:\n",
    "    \"\"\"\n",
    "    Check if a policy is already contained in the list of policies.\n",
    "\n",
    "    Args:\n",
    "        policy (dict): Policy to check.\n",
    "        all_policies (dict): List of all policies, per base policy.\n",
    "    Returns:\n",
    "        bool: True if the policy is already contained, False otherwise.\n",
    "    \"\"\"\n",
    "    for next_policies in all_policies.values():\n",
    "        if contains_policy(next_policies.values(), policy):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "next_policies_per_policy = {}\n",
    "for policy_name, signature in signature_per_policy.items():\n",
    "    print(f\"Base policy: {policy_name}\")\n",
    "    print()\n",
    "\n",
    "    policies = {}\n",
    "    next_policies_per_policy[policy_name] = policies\n",
    "    for flow in signature.get_flows():\n",
    "        # Extract policy\n",
    "        policy = flow.extract_policy(DEVICE_IP)\n",
    "\n",
    "        # Check if policy is already contained in the list of policies\n",
    "        if policy_exists(policy, next_policies_per_policy):\n",
    "            continue\n",
    "\n",
    "        # Policy does not exist yet,\n",
    "        # add it to the list of policies\n",
    "        policy_name = flow.get_id()\n",
    "        policies[policy_name] = policy\n",
    "\n",
    "        # Display policy\n",
    "        print(f\"Policy {policy_name}:\")\n",
    "        print(json.dumps(policy, indent=2))\n",
    "        print()\n",
    "    \n",
    "    print()\n",
    "\n",
    "pprint(next_policies_per_policy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418587bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Translate list of policies to the corresponding NFTables and NFQueue files.\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "from profile_translator_blocklist import translate_policy\n",
    "\n",
    "\n",
    "def find_existing_dir(dir_base: os.PathLike, policy_name: str) -> os.PathLike:\n",
    "    \"\"\"\n",
    "    Find an existing directory,\n",
    "    for which the name matches the given policy name,\n",
    "    suffixed with a timestamp (a number).\n",
    "\n",
    "    Args:\n",
    "        dir_base (os.PathLike): Base directory to search in.\n",
    "        policy_name (str): Name of the policy to match.\n",
    "    Returns:\n",
    "        os.PathLike: Path to the existing directory, or None if not found.\n",
    "    \"\"\"\n",
    "    pattern = re.compile(rf\"{re.escape(policy_name)}_\\d+\")\n",
    "    for entry in os.scandir(dir_base):\n",
    "        if entry.is_dir() and pattern.match(entry.name):\n",
    "            return entry.path\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "device = {\n",
    "    \"name\": DEVICE_NAME,\n",
    "    \"mac\":  DEVICE_MAC,\n",
    "    \"ipv4\": DEVICE_IP\n",
    "}\n",
    "\n",
    "for base_policy_name, next_policies in next_policies_per_policy.items():\n",
    "    depth = int(base_policy_name.partition(\"_\")[0])\n",
    "    for name, policy in next_policies.items():\n",
    "        prefix_dir = f\"{depth + 1}_{name}\"\n",
    "\n",
    "        # Find existing policy directory with timestamp\n",
    "        existing_dir = find_existing_dir(EVENT_DIR, prefix_dir)\n",
    "\n",
    "        # No existing directory found, create a new one\n",
    "        if existing_dir is None:\n",
    "            existing_dir = prefix_dir\n",
    "            os.makedirs(existing_dir, exist_ok=True)\n",
    "\n",
    "        policy_dir = os.path.join(EVENT_DIR, existing_dir)\n",
    "        translate_policy(device, policy, nfqueue_name=name, output_dir=policy_dir)\n",
    "        print(f\"Wrote to {policy_dir}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
